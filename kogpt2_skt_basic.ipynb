{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ec7e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/danbi/userdata/SGM_AI/darklady'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef95a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install ipywidgets\n",
    "!pip install scikit-learn\n",
    "!pip install whatlies[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0a268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 22 15:11:55 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A5000    On   | 00000000:02:00.0 Off |                  Off |\r\n",
      "| 30%   34C    P8    15W / 230W |      8MiB / 24564MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29cb629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding =\"UTF-8\") as f:\n",
    "    #with open(file_path, 'r') as f:\n",
    "        fr = f.read()\n",
    "    return fr\n",
    "\n",
    "def read_csv(file_path):\n",
    "    fr = pd.read_csv(file_path, encoding = \"utf-8\")\n",
    "    return fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db7222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>동화</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022년 5월 5일은 방정환 선생님이 어린이날을 만든 지 100주년이 되는 날이랍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5월은 가정의 달이라고 할 만큼 가족들의 소중함을 생각해보게 하는 기념일이 참 많은...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>우리 친구들은 또래 친구들을 만나면 보통 어떤 놀이를 하면서 시간을 보내나요?  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6월 6일은 나라를 지키기 위해 목숨을 바친 모든 이들의 넋을 기리는 법정 기념일 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1309</td>\n",
       "      <td>옛날에 한 번은 마부가 포도주를 잔득 실은 ‘달구지’를 끌고 가다 그만 속도를 너무...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>1310</td>\n",
       "      <td>어느 커다란 도시에 나이 드신 어머님 한 분이 자신의 방에 저녁 내내 앉아 그동안 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1311</td>\n",
       "      <td>가난한 시골 소년이 어느 날 교회에서 목사님이 “하늘나라 왕국에 들어가길 원하는 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1312</td>\n",
       "      <td>어느 날 오후에 아기 예수가 아기침대에 누워 잠을 자고 있었답니다. 그때 그의 어머...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>1313</td>\n",
       "      <td>하루는 노인과 그의 아내가 누추한 집 앞에 앉아 잠시 일손을 놓고 휴식들을 취하고 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                 동화\n",
       "0              0   우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 ...\n",
       "1              1  2022년 5월 5일은 방정환 선생님이 어린이날을 만든 지 100주년이 되는 날이랍...\n",
       "2              2  5월은 가정의 달이라고 할 만큼 가족들의 소중함을 생각해보게 하는 기념일이 참 많은...\n",
       "3              3   우리 친구들은 또래 친구들을 만나면 보통 어떤 놀이를 하면서 시간을 보내나요?  ...\n",
       "4              4  6월 6일은 나라를 지키기 위해 목숨을 바친 모든 이들의 넋을 기리는 법정 기념일 ...\n",
       "...          ...                                                ...\n",
       "1309        1309  옛날에 한 번은 마부가 포도주를 잔득 실은 ‘달구지’를 끌고 가다 그만 속도를 너무...\n",
       "1310        1310  어느 커다란 도시에 나이 드신 어머님 한 분이 자신의 방에 저녁 내내 앉아 그동안 ...\n",
       "1311        1311  가난한 시골 소년이 어느 날 교회에서 목사님이 “하늘나라 왕국에 들어가길 원하는 자...\n",
       "1312        1312  어느 날 오후에 아기 예수가 아기침대에 누워 잠을 자고 있었답니다. 그때 그의 어머...\n",
       "1313        1313  하루는 노인과 그의 아내가 누추한 집 앞에 앉아 잠시 일손을 놓고 휴식들을 취하고 ...\n",
       "\n",
       "[1314 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = read_csv('/home/danbi/userdata/SGM_AI/darklady/dataset/dataset_0815.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0ea627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>동화</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022년 5월 5일은 &lt;인물1&gt; 선생님이 어린이날을 만든 지 100주년이 되는 날...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5월은 가정의 달이라고 할 만큼 가족들의 소중함을 생각해보게 하는 기념일이 참 많은...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>우리 친구들은 또래 친구들을 만나면 보통 어떤 놀이를 하면서 시간을 보내나요?  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6월 6일은 나라를 지키기 위해 목숨을 바친 모든 이들의 넋을 기리는 법정 기념일 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1309</td>\n",
       "      <td>옛날에 한 번은 마부가 포도주를 잔득 실은 ‘달구지’를 끌고 가다 그만 속도를 너무...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>1310</td>\n",
       "      <td>어느 커다란 도시에 나이 드신 어머님 한 분이 자여신의 방에 저녁 내내 앉아 그동안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1311</td>\n",
       "      <td>가난한 시골 소녀가 어느 날 교회에서 목사님이 “하늘나라 왕국에 들어가길 원하는 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1312</td>\n",
       "      <td>어느 날 오후에 아기 예수가 아기침대에 누워 잠을 자고 있었답니다. 그때 *그녀의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>1313</td>\n",
       "      <td>하루는 노인과 *그녀의 남편이 누추한 집 앞에 앉아 잠시 일손을 놓고 휴식들을 취하...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                 동화\n",
       "0              0   우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 ...\n",
       "1              1  2022년 5월 5일은 <인물1> 선생님이 어린이날을 만든 지 100주년이 되는 날...\n",
       "2              2  5월은 가정의 달이라고 할 만큼 가족들의 소중함을 생각해보게 하는 기념일이 참 많은...\n",
       "3              3   우리 친구들은 또래 친구들을 만나면 보통 어떤 놀이를 하면서 시간을 보내나요?  ...\n",
       "4              4  6월 6일은 나라를 지키기 위해 목숨을 바친 모든 이들의 넋을 기리는 법정 기념일 ...\n",
       "...          ...                                                ...\n",
       "1309        1309  옛날에 한 번은 마부가 포도주를 잔득 실은 ‘달구지’를 끌고 가다 그만 속도를 너무...\n",
       "1310        1310  어느 커다란 도시에 나이 드신 어머님 한 분이 자여신의 방에 저녁 내내 앉아 그동안...\n",
       "1311        1311  가난한 시골 소녀가 어느 날 교회에서 목사님이 “하늘나라 왕국에 들어가길 원하는 자...\n",
       "1312        1312  어느 날 오후에 아기 예수가 아기침대에 누워 잠을 자고 있었답니다. 그때 *그녀의 ...\n",
       "1313        1313  하루는 노인과 *그녀의 남편이 누추한 집 앞에 앉아 잠시 일손을 놓고 휴식들을 취하...\n",
       "\n",
       "[1314 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapped_dataset = read_csv('/home/danbi/userdata/SGM_AI/darklady/dataset/swapped_data_0822.csv')\n",
    "swapped_dataset#['동화']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9ce800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 조각 작품을 만드는 것 그런데 지금의 미술은 꼭 그림이나 조각품만이 아니랍니다.  비디오 아트 행위 예술 벽의 낙서까지 그 어떤 것도 예술이고 미술일 수 있다고 해요. 그럼 오늘 신나는 동화여행 속에서 알쏭달쏭 현대미술관으로 재미있고 엉뚱한 작품을 만나러 가볼까요.  알쏭달쏭 미술관 \"나는 알쏭달쏭 미술관 관장 \\'다다\\'란다 이곳은 신기하고 괴상한 작품으로 가득하지 재미있는 미술 작품들이 가득한 알쏭달쏭 미술관에 온 걸 환영합니다.\" \"이쪽에 있는 변기를 볼래? 화장실에서나 볼 수 있는 변기가 미술 작품이라면 믿겠니?\" \"네? 변기가 미술 작품이라구요?\" \"너희들 모나리자란 그림을 알고 있지? 레오나르도 다빈치가 그린 그림이야. 그런데 뒤샹이라는 화가는 모나리자의 얼굴에 수염만 그리고 전시를 했단다. 그래서 사람들은 깜짝 놀랐지.\" \"네? 유명한 작품에 낙서를 한 게 미술 작품이라구요?\" \"뒤샹은 장난꾸러기였나봐. 그럼 이거는 뭘 그린 건지 한번 맞춰볼래? 여러 색깔의 물감을 뿌리고 붓고 흘린 거란다 곳곳에 커다란 물감 얼룩도 있지\" \"폴록이라는 화가는 이게 연보라빛 안개라는구나. 그런데 미술관 관장인 내가 봐도 신나게 물감을 뿌려 놓은 것 같거든 혹시 너희들 눈에는 안개가 보이니?\" \"글쎄요? 보이는 것도 같고 아닌 것 같기도 하고..\" \"그렇게 볼록은 커다란 종류의 물감을 쏟아 부었어. 그리고 붓을 휘젓고 물감을 튀겼지. 가끔은 물감 대신 모래를 톡톡 뿌리기도 했단다. 진짜 신났겠지?\" \"물감을 가지고 장난치는 것 같아...\" \"화가 에른스트는 물건의 종이를 대고 문지르면서 작품을 만들었어. 어떤 물건이냐에 따라 무늬가 달라지지!\" \"에이 이건 나도 그릴 수 있을 것 같아.\" \"종이에 물감을 쭉 짜놓고 접었다가 펴면 양쪽에 똑같은 그림이 나타나지 이걸 \\'데칼코마니\\'라고 한단다.\" \"이 그림은 어떠냐 이것도 미술 작품이야. 리엔텐 슈타인은 만화의 한 장면에 검은 선을 그린 다음 알록달록 색칠을 했단다 진짜 만화책처럼 보이려고 촘촘히 작은 점도 찍었어.\"  \"관장님 저도 만화책으로 작품을 만들어 볼래요!\" \"그리고 리히텐슈타인은 말풍선 안에 글도 넣었단다. 누구나 알기 쉽게 재미있는 그림을 그리고 싶었던 거야 어때 만화를 보는 것처럼 쉽지?\" \"네 꼭 만화책 같아요.\" \"햄버거와 막대 아이스크림을 미술 작품이라고 전시한 미술가도 있단다. 먹고 싶다기보다는 깔고 앉고 싶지.\" \"물론 절대로 먹을 수는 없어 고무와 상자 천으로 만들어졌거든.\" \"야 이런 것도 현대미술이구나. 어 갑자기 배가 고파져.\" \"화가 올덴버그는 반대로 생각하는 걸 좋아했단다. 사람이나 눈물 대신 물건이나 음식을 조각으로 만들었어. \"숟가락, 미끄럼틀, 먹다 버린 사과, 배드민턴 공 이런 재료들이 모두 미술이 되었지 재료도 이것저것 써서 작은 곳은 엄청 크게 단단한 곳은 물렁물렁하게 만들었단다.\" \"먹다 버린 사과가 멋진 작품이 된 거야.\"  \"심심해서 끄적거리는 것도 미술이 될까?\" \"화가 바스키아는 지하철과 길거리 벽에 글을 쓰고 그림을 그렸어. 아프리카 가면을 닮은 얼굴이랑 글자 화려한 색깔과 모양이 어우러져 활기찬 그림이 되었지.\" \"낙서가 예술 작품이 될 수 있다구요? 어 그럼 나도 벌써 예술간데..?\" \"스미스슨이라는 화가는 호수의 빙글빙글 달팽이 집 모양 둑을 만들었어. 이건 비행기를 타야 볼 수 있겠지.  \"저기요 저기 멋진 둑이 보여요.\" \"이 둑은 물에 잠기기도 하고 부서지거나 깎이면서 조금씩 없어진단다. 이런 것까지도 작품이라고 생각했단다. 현대 미술가들은 톡톡 튀는 생각 톡톡 튀는 방법으로 작품을 만들어요.\" \"허스트는 죽은 동물이나 알약으로 죽음을 표현했단다.\" \"그런데 이게 모두 미술이 맞는 걸까?\"  \"미술이 그림을 그리는 것만은 아닌가 봐요. 그래 맞아 현대 미술가들은 자기가 그리고 싶은 대로 그리고 만들고 싶은 대로 만들지 우리는 그냥 보이는 대로 느끼면 된단다.\" 정말 생각지도 않았던 것들이 미술 작품으로 전시되어 있었어요. 이렇게 현대미술은 표현 방법이나 주제 또 재료와 도구들이 아주 아주 다양해졌답니다. 우리 친구들도 느끼는 대로 생각하는 대로 머릿속에 그려지는 상상력을 마음껏 나타내 보세요. 바로 그게 미술이니까요. 그럼 우리는 다음에 또 다른 이야기로 다시 만나요. 안녕~ 우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 조각 작품을 만드는 것 그런데 지금의 미술은 꼭 그림이나 조각품만이 아니랍니다.  비디오 아트 행위 예술 벽의 낙서까지 그 어떤 것도 예술이고 미술일 수 있다고 해요. 그럼 오늘 신나는 동화여행 속에서 알쏭달쏭 현대미술관으로 재미있고 엉뚱한 작품을 만나러 가볼까요.  알쏭달쏭 미술관 \"나는 알쏭달쏭 미술관 관장 \\'<인물0>\\'란다 이곳은 신기하고 괴상한 작품으로 가득하지 재미있는 미술 작품들이 가득한 알쏭달쏭 미술관에 온 걸 환영합니다.\" \"이쪽에 있는 변기를 볼래? 화장실에서나 볼 수 있는 변기가 미술 작품이라면 믿겠니?\" \"네? 변기가 미술 작품이라구요?\" \"너희들 모나리자란 그림을 알고 있지? 레오나르도 다빈치가 그린 그림이야. 그런데 뒤샹이라는 화가는 모나리자의 얼굴에 수염만 그리고 전시를 했단다. 그래서 사람들은 깜짝 놀랐지.\" \"네? 유명한 작품에 낙서를 한 게 미술 작품이라구요?\" \"뒤샹은 장난꾸러기였나봐. 그럼 이거는 뭘 그린 건지 한번 맞춰볼래? 여러 색깔의 물감을 뿌리고 붓고 흘린 거란다 곳곳에 커다란 물감 얼룩도 있지\" \"폴록이라는 화가는 이게 연보라빛 안개라는구나. 그런데 미술관 관장인 내가 봐도 신나게 물감을 뿌려 놓은 것 같거든 혹시 너희들 눈에는 안개가 보이니?\" \"글쎄요? 보이는 것도 같고 아닌 것 같기도 하고..\" \"그렇게 볼록은 커다란 종류의 물감을 쏟아 부었어. 그리고 붓을 휘젓고 물감을 튀겼지. 가끔은 물감 대신 모래를 톡톡 뿌리기도 했단다. 진짜 신났겠지?\" \"물감을 가지고 장난치는 것 같아...\" \"화가 에른스트는 물건의 종이를 대고 문지르면서 작품을 만들었어. 어떤 물건이냐에 따라 무늬가 달라지지!\" \"에이 이건 나도 그릴 수 있을 것 같아.\" \"종이에 물감을 쭉 짜놓고 접었다가 펴면 양쪽에 똑같은 그림이 나타나지 이걸 \\'데칼코마니\\'라고 한단다.\" \"이 그림은 어떠냐 이것도 미술 작품이야. 리엔텐 슈타인은 만화의 한 장면에 검은 선을 그린 다음 알록달록 색칠을 했단다 진짜 만화책처럼 보이려고 촘촘히 작은 점도 찍었어.\"  \"관장님 저도 만화책으로 작품을 만들어 볼래요!\" \"그리고 리히텐슈타인은 말풍선 안에 글도 넣었단다. 누구나 알기 쉽게 재미있는 그림을 그리고 싶었던 거야 어때 만화를 보는 것처럼 쉽지?\" \"네 꼭 만화책 같아요.\" \"햄버거와 막대 아이스크림을 미술 작품이라고 전시한 미술가도 있단다. 먹고 싶다기보다는 깔고 앉고 싶지.\" \"물론 절대로 먹을 수는 없어 고무와 상자 천으로 만들어졌거든.\" \"야 이런 것도 현대미술이구나. 어 갑자기 배가 고파져.\" \"화가 올덴버*그녀는 반대로 생각하는 걸 좋아했단다. 사람이나 눈물 대신 물건이나 음식을 조각으로 만들었어. \"숟가락, 미끄럼틀, 먹다 버린 사과, 배드민턴 공 이런 재료들이 모두 미술이 되었지 재료도 이것저것 써서 작은 곳은 엄청 크게 단단한 곳은 물렁물렁하게 만들었단다.\" \"먹다 버린 사과가 멋진 작품이 된 거야.\"  \"심심해서 끄적거리는 것도 미술이 될까?\" \"화가 바스키아는 지하철과 길거리 벽에 글을 쓰고 그림을 그렸어. 아프리카 가면을 닮은 얼굴이랑 글자 화려한 색깔과 모양이 어우러져 활기찬 그림이 되었지.\" \"낙서가 예술 작품이 될 수 있다구요? 어 그럼 나도 벌써 예술간데..?\" \"스미스슨이라는 화가는 호수의 빙글빙글 달팽이 집 모양 둑을 만들었어. 이건 비행기를 타야 볼 수 있겠지.  \"저기요 저기 멋진 둑이 보여요.\" \"이 둑은 물에 잠기기도 하고 부서지거나 깎이면서 조금씩 없어진단다. 이런 것까지도 작품이라고 생각했단다. 현대 미술가들은 톡톡 튀는 생각 톡톡 튀는 방법으로 작품을 만들어요.\" \"허스트는 죽은 동물이나 알약으로 죽음을 표현했단다.\" \"그런데 이게 모두 미술이 맞는 걸까?\"  \"미술이 그림을 그리는 것만은 아닌가 봐요. 그래 맞아 현대 미술가들은 자기가 그리고 싶은 대로 그리고 만들고 싶은 대로 만들지 우리는 그냥 보이는 대로 느끼면 된단다.\" 정말 생각지도 않았던 것들이 미술 작품으로 전시되어 있었어요. 이렇게 현대미술은 표현 방법이나 주제 또 재료와 도구들이 아주 아주 다양해졌답니다. 우리 친구들도 느끼는 대로 생각하는 대로 머릿속에 그려지는 상상력을 마음껏 나타내 보세요. 바로 그게 미술이니까요. 그럼 우리는 다음에 또 다른 이야기로 다시 만나요. 안녕~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset = dataset + swapped_dataset\n",
    "concat_dataset['동화'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914272bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(concat_dataset['동화'], test_size=0.2, shuffle=True, random_state=123)\n",
    "#trainset, testset\n",
    "\n",
    "fw = open(\"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_trainset.txt\", \"w\", encoding = \"utf-8\")\n",
    "fw.write(trainset.str.cat(sep=' ').replace('\\n', ' '))\n",
    "fw.close()\n",
    "\n",
    "fw = open(\"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_testset.txt\", \"w\", encoding = \"utf-8\")\n",
    "fw.write(testset.str.cat(sep=' ').replace('\\n', ' '))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c7377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "# from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\", use_cache = False,\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', use_cache = False).to(device='cuda', non_blocking=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47084906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 옛날 옛적에 한 오누이가 살고 있었습니다. 오누이는 뚱뚱하고 마른데다가 옷이 좀 무거웠습니다. 남편은 다 굶어 봤습니다. 그런데 오누이가 어떻게 되었는지 보려고 뚱뚱한 오누이를 보았습니다. 오누이도 키가 큰 데다가 늘씬한 체형의 몸이라 체중이 줄면 살이 찌기 쉽다고 들었습니다. 뚱뚱한 오누이를 보니 너무 큰 것이 아닐까 생각하였습니다. 오누이는 키가 크고 마른데다가 살이 찌는 것 같았습니다. 그래서 저는 몸을 늘여보고 다리를 늘여보고 다리를 늘여보고 다리를 늘여보고 다리를 늘여보는 등의 방법을 반복하여 보았습니다. 그러다가 보니 오누이는 키가 크고 마른데도 살이 찌었습니다. 이렇게 살이 찌는 것을 볼 수 있는 방법은 바로 다리를 늘여 보고 다리를 늘여보는 것이었습니다. 다리를 늘여보고 다리를 늘여보는 방법으로 다이어트에 성공한 것입니다. 저희 부부는 그 당시 저의 몸무게가 얼마나 줄었는지 모르지만 당시에는 제 모습이 아니었습니다. 살과 살의 관계를 보고 나니 가슴이 뭉클하였습니다. 이렇게 생각하니 몸도 커지고 몸이 작아졌다. 그러자 그제야 오누이는 조금 더 살이 쏙 빠지면서\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "옛날 옛적에 한 오누이가 살고 있었습니다.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "  tokens_bfr = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens_bfr = model.generate(tokens_bfr, do_sample=True, temperature=0.9, max_length=250, \n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              bos_token_id=tokenizer.bos_token_id)\n",
    "  generated_bfr = tokenizer.batch_decode(gen_tokens_bfr)[0].replace(\"\\n\", \" \")\n",
    "  \n",
    "print(generated_bfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff293123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_path = \"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_trainset.txt\"\n",
    "test_path = \"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_testset.txt\"\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76905fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "21c4d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0822\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=60, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    logging_steps=20,\n",
    "    eval_steps = 100, # Number of update steps between two evaluations.\n",
    "    save_steps= 15000, # after # steps model is saved \n",
    "    warmup_steps=400,# number of warmup steps for learning rate scheduler\n",
    "    save_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "769cebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16893\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10342' max='15840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10342/15840 1:20:49 < 42:58, 2.13 it/s, Epoch 39.17/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>5.906689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>5.906446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>5.929402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>5.927177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>5.923293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>5.956725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>5.961024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>5.969594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>5.999148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>5.999606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>6.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>6.022871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>6.025768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>6.058149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>6.071421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>6.080831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>6.094381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>6.106766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>6.123471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>6.154196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>6.156910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>6.175467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>6.186993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>6.195515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>6.216590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>6.233203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>6.247807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>6.254884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>6.259809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>6.290055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>6.309894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>6.315757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>6.331435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>6.345001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>6.361568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>6.360255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>6.368968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>6.387169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>6.401434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.150800</td>\n",
       "      <td>6.409692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>6.429729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>6.437250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>6.461746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>6.459434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>6.472706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>6.497109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>6.490188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>6.518878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>6.522141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>6.521810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>6.550513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>6.559628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>6.551372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>6.582603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>6.576531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>6.589145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>6.591374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>6.603656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>6.622678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>6.623739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>6.642147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>6.658638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>6.653213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>6.659031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>6.667568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>6.669468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>6.688904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>6.695778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>6.715701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>6.727272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>6.725380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>6.736476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>6.739078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>6.770044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>6.768599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>6.761606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>6.773986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>6.786847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>6.788772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>6.813533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.096300</td>\n",
       "      <td>6.805608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>6.823657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>6.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>6.823407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>6.845469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>6.836195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>6.855341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>6.859568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>6.856849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>6.874596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>6.879575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>6.889223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>6.891972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>6.891950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>6.896856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>6.908528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>6.909407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>6.918198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.908295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>6.936914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>6.930692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>6.943425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.936104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "#optimizer.zero_grad()\n",
    "#with torch.no_grad():\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b2bb500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819\n",
      "Configuration saved in /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/config.json\n",
      "Model weights saved in /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bdc034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_path = \"--\"\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path).to(device='cuda', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d4c90b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 옛날 옛적에 한 오누이가 살고 있었어요.  오빠는 너무도 가난하여 양식을 구할 길이 없어 아들아이를 가지길 간절히 바랬고, 자카따룹은 세상을 떠나고 말았어요. 그때 우연히 엄마 나나가 할머니에게 말하는 것을 얻었지요. “할머니, 저하고 결혼하세요.” 사랑하는 아빠를 섬기다 가버린 애가 하루는 꿈에 삼았던 그 새가 살아 있는 걸 보았답니다. 그녀가 창가로 달려가 정말 어느 새 큰 원 안에 갇혀 있게 되었는지 궁금해졌거든요.  그러자 왕비가 이야기를 시작했습니다. “이제 떠돌아다니는 삶에도 지쳤소. 그러니 내가 하느님께 나아갈 수 있도록 도와다오.” 공주가 대답하였어.  “아니,”라며 왕이 자기와 함께 동행한 아기 곰을 데려오셨는데, 그도 마찬가지로 구혼자들이 나타나 이렇게 말하며 그가 원하는 방향으로 헤엄쳐 갔다는 거예요, 그래서 그는 마침내 그를 엄마와 아이를 살려줄 용으로 변신시키고 왕비와 나누어 달라 청하리라 부르게 된 거지 뭐죠. 하지만 이 일로 왕비는 납북절한테는 어떤 약속 말고 그냥 묻어주시기만 하면 된다는 뜻이었습니다.  넌 분명 아름답고 영리한테서 그리고 아무 것도 얻을 수가 있는데 말이 있어 그리할 것이며, 그렇다면 그렇게 말하고 싶진 않으냐 그러면서\n"
     ]
    }
   ],
   "source": [
    "#existing code\n",
    "prompt = \"\"\"\n",
    "옛날 옛적에 한 오누이가 살고 있었어요.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = finetuned_model.generate(tokens, \n",
    "                                        early_stopping=True,\n",
    "                                        do_sample=True,\n",
    "                                        temperature=0.8, \n",
    "                                        max_length=250, \n",
    "                                        repetition_penalty=3.0,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        unk_token_id=tokenizer.unk_token_id,)\n",
    "  generated = tokenizer.decode(gen_tokens[0], skip_special_tokens=True).replace(\"\\n\", \" \")\n",
    "  \n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
